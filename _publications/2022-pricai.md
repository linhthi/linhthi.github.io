---
title: "Dynamic-GTN: Learning an Node Efficient Embedding in Dynamic Graph with Transformer"
collection: publications
permalink: /publication/2022-pricai
excerpt: 'Graph Transformer Networks (GTN) use an attention mechanism to learn the node representation in a static graph and achieves state-of-the-art results on several graph learning tasks. However, due to the computation complexity of the attention operation, GTNs are not applicable to dynamic graphs. In this paper, we propose the Dynamic-GTN model which is designed to learn the node embedding in a continous-time dynamic graph. The Dynamic-GTN extends the attention mechanism in a standard GTN to include temporal information of recent node interactions. Based on temporal patterns interaction between nodes, the Dynamic-GTN employs an node sampling step to reduce the number of attention operations in the dynamic graph. We evaluate our model on three benchmark datasets for learning node embedding in dynamic graphs. The results show that the Dynamic-GTN has better accuracy than the state-of-the-art of Graph Neural Networks on both transductive and inductive graph learning tasks.'
date: 2022-11-4
venue: 'Pacific Rim International Conference on Artificial Intelligence (PRICAI) 2022'
# paperurl: '[PDF](https://eprints.uet.vnu.edu.vn/eprints/id/eprint/4688/2/Improving_Graph_Convolutional_Networks_with_Transformer_Layer_in_social-based_items_recommendation_IEEE_Express.pdf)'
citation: 'T. L. Hoang, and V. C. Ta, "Dynamic-GTN: Learning an Node Efficient Embedding in Dynamic Graph with Transformer", PRICAI 2022.'
---
With the emergence of online social networks, social-based items recommendation has become a popular research direction. Recently, Graph Convolutional Networks have shown promising results by modeling the information diffusion process in graphs. It provides a unified framework for graph embedding that can leverage both the social graph structure and node features information. In this paper, we improve the embedding output of the graph-based convolution layer by adding a number of transformer layers. The transformer layers with attention architecture help discover frequent patterns in the embedding space which increase the predictive power of the model in the downstream tasks. Our approach is tested on two social-based items recommendation datasets, Ciao and Epinions and our model outperforms other graph-based recommendation baselines.

[Download paper here]https://eprints.uet.vnu.edu.vn/eprints/id/eprint/4776/1/PRICAI_2022_paper_119%20%281%29.pdf)

Recommended citation: 'T. L. Hoang, and V. C. Ta, "Dynamic-GTN: Learning an Node Efficient Embedding in Dynamic Graph with Transformer", PRICAI 2022.'.